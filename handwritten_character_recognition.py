# -*- coding: utf-8 -*-
"""Handwritten Character Recognition

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V5SF0QvaKlvxisKBRRAOk_oVMvzh0yPE
"""

# Colab-ready: Handwritten Character Recognition (MNIST + EMNIST Letters)
# TensorFlow 2.x, tfds
# Paste into a Colab cell and run. Enable GPU runtime for faster training.

# 0) Install / Imports
!pip install -q --upgrade tensorflow tensorflow-datasets matplotlib scikit-learn

import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import itertools
import os
tf.random.set_seed(42)

# 1) Settings
BATCH_SIZE = 128
IMG_SIZE = (28, 28)
AUTOTUNE = tf.data.AUTOTUNE
EPOCHS = 12
USE_EMNIST = True  # set False to use only MNIST digits

# 2) Helper preprocess functions
def preprocess_mnist(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    # expand channel if needed
    if image.shape.rank == 2:
        image = tf.expand_dims(image, -1)
    return image, label

def preprocess_emnist(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    # Fix orientation
    image = tf.transpose(image, perm=[1, 0, 2])
    image = tf.image.flip_left_right(image)
    if image.shape.rank == 2:
        image = tf.expand_dims(image, -1)
    label = label - 1  # ðŸ”‘ shift labels from [1..26] â†’ [0..25]
    return image, label


# 3) Load datasets
# MNIST (digits)
mnist_train = tfds.load("mnist", split="train", as_supervised=True)
mnist_test  = tfds.load("mnist", split="test",  as_supervised=True)

mnist_train = mnist_train.map(preprocess_mnist, num_parallel_calls=AUTOTUNE)
mnist_test  = mnist_test.map(preprocess_mnist, num_parallel_calls=AUTOTUNE)

mnist_train = mnist_train.cache().shuffle(10000).batch(BATCH_SIZE).prefetch(AUTOTUNE)
mnist_test  = mnist_test.batch(BATCH_SIZE).prefetch(AUTOTUNE)

# EMNIST letters (optional)
# EMNIST letters (optional)
if USE_EMNIST:
    # Several EMNIST variants exist. We use the 'letters' split (26 classes).
    emnist_train = tfds.load("emnist/letters", split="train", as_supervised=True)
    emnist_test  = tfds.load("emnist/letters", split="test",  as_supervised=True)
    emnist_train = emnist_train.map(preprocess_emnist, num_parallel_calls=AUTOTUNE)
    emnist_test  = emnist_test.map(preprocess_emnist, num_parallel_calls=AUTOTUNE)
    emnist_train = emnist_train.cache().shuffle(10000).batch(BATCH_SIZE).prefetch(AUTOTUNE)
    emnist_test  = emnist_test.batch(BATCH_SIZE).prefetch(AUTOTUNE)

# 4) Choose which dataset to train on (switch variable)
TRAIN_ON = "emnist"  # choices: "mnist" or "emnist"
if TRAIN_ON == "mnist":
    train_ds = mnist_train
    val_ds   = mnist_test
    num_classes = 10
    class_names = [str(i) for i in range(10)]
else:
    if not USE_EMNIST:
        raise RuntimeError("EMNIST not loaded. Set USE_EMNIST = True above.")
    train_ds = emnist_train
    val_ds   = emnist_test
    # EMNIST letters: tfds provides info -> class labels 1..26 sometimes; we'll map to 'A'..'Z'
    num_classes = 26
    class_names = [chr(ord('A') + i) for i in range(num_classes)]

print("Num classes:", num_classes)
print("Example classes:", class_names[:6])

# 5) Build a robust CNN model
def make_cnn_model(input_shape=(28,28,1), num_classes=10):
    inputs = tf.keras.Input(shape=input_shape)
    x = inputs
    x = tf.keras.layers.Conv2D(32, 3, activation="relu", padding="same")(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.MaxPool2D(2)(x)
    x = tf.keras.layers.Conv2D(64, 3, activation="relu", padding="same")(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.MaxPool2D(2)(x)
    x = tf.keras.layers.Conv2D(128, 3, activation="relu", padding="same")(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(128, activation="relu")(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    outputs = tf.keras.layers.Dense(num_classes, activation="softmax")(x)
    model = tf.keras.Model(inputs, outputs)
    return model

model = make_cnn_model((28,28,1), num_classes)
model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)
model.summary()

# 6) Callbacks
callbacks = [
    tf.keras.callbacks.ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=2, min_lr=1e-6),
    tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=4, restore_best_weights=True)
]

# 7) Train
history = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=callbacks)

# 8) Evaluate
loss, acc = model.evaluate(val_ds)
print(f"Test loss: {loss:.4f}, Test accuracy: {acc:.4f}")

# 9) Confusion Matrix & Classification Report (on test set)
# Collect all predictions and labels (be careful with memory for big datasets)
y_true = []
y_pred = []

for images, labels in val_ds:
    preds = model.predict(images)
    preds = np.argmax(preds, axis=1)
    y_true.append(labels.numpy())
    y_pred.append(preds)

y_true = np.concatenate(y_true)
y_pred = np.concatenate(y_pred)

cm = confusion_matrix(y_true, y_pred)
print("Classification report:")
print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))

# Plot confusion matrix (normalized)
def plot_confusion_matrix(cm, classes, normalize=True, title="Confusion matrix"):
    if normalize:
        cm = cm.astype("float") / (cm.sum(axis=1)[:, np.newaxis] + 1e-12)
    plt.figure(figsize=(10,8))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90)
    plt.yticks(tick_marks, classes)
    fmt = ".2f" if normalize else "d"
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

plot_confusion_matrix(cm, class_names)

# 10) Show sample predictions
def show_samples(dataset, class_names, n=12):
    # get one batch
    for images, labels in dataset.take(1):
        preds = model.predict(images)
        preds = np.argmax(preds, axis=1)
        plt.figure(figsize=(12,6))
        for i in range(n):
            ax = plt.subplot(3, 4, i+1)
            img = images[i].numpy().squeeze()
            plt.imshow(img, cmap="gray")
            plt.title(f"GT:{class_names[int(labels[i])]} / P:{class_names[int(preds[i])]}")
            plt.axis("off")
        plt.show()

show_samples(val_ds, class_names, n=12)

# 11) Save model
save_path = "/content/handwritten_cnn_emnist.h5"
model.save(save_path)
print("Model saved to:", save_path)

# 12) Notes: Extending to CRNN for full-word recognition (outline)
crnn_notes = """
To extend to full-word / sentence recognition (CRNN):
1. Use line-level images (words) instead of single characters.
2. Front-end CNN extracts feature maps.
3. Collapse width dimension and feed sequence to a BiLSTM (or GRU).
4. Use a final Dense layer + CTC loss for training (Connectionist Temporal Classification).
5. At inference, use CTC beam search decoding to get text output.
Common libs/examples: keras-ocr, TensorFlow OCR tutorials, or implement custom model using tf.nn.ctc_loss.
"""
print(crnn_notes)

# 13) Quick tips:
print("""
Tips:
- If EMNIST orientation looks off, double-check the transpose+flip above.
- Try data augmentation (rotation, shift) to improve generalization.
- For small GPU/Colab runs, reduce batch size or epochs if running out of memory.
- To use GPU on Colab: Runtime -> Change runtime type -> GPU.
""")

